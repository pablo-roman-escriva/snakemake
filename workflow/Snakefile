import os
import shutil

report: "report/workflow.rst"

configfile: "config/config.yaml"
input_dir = config["folders"]["input"]
intermediate_dir = config["folders"]["intermediate"]
output_dir = config["folders"]["output"]

samples, = glob_wildcards(input_dir+"/{id}.fastq")
filter_type = ["filtered", "unfiltered"]
tax_type_counts = ["species", "genus"]
if (sum(len(files) for _, _, files in os.walk(input_dir)) == 1):
    graph_type = ["most_abundant"]
else: 
    graph_type = ["most_abundant", "clustered_heatmap"]
tax_level = ["superkingdom","phylum","class","order","family","genus","species"]
number = {}

def graph_label(tipo):
    if tipo == "clustered_heatmap":
        return "Heatmap"
    elif tipo == "most_abundant":
        return "Barplot"
    else:
        return "Unknown"

def dict_labels(wildcards): 
    my_dict = {
        "Graph" : graph_label(wildcards.graph_type)
    }
    return my_dict

def dict_labels_filter_type(wildcards):
    my_dict = {
        "Type": wildcards.filter_type.capitalize()
    }
    return my_dict

def dict_labels_filter_tax(wildcards):
    my_dict = {
        "Type": wildcards.filter_type.capitalize() + " " + wildcards.tax_type_counts
    }
    return my_dict

rule all:
    input:
        intermediate_dir+"/read_counts.xlsx",
        expand(intermediate_dir+"/preqc/{id}_fastqc.html", id=samples),
        expand(intermediate_dir+"/postqc/{id}_fastqc.html", id=samples),
        expand(intermediate_dir+"/counts_{filter_type}_{tax_type_counts}.tsv", filter_type=filter_type, tax_type_counts=tax_type_counts),
        expand(intermediate_dir+"/krona_{filter_type}/krona_composition_samples.html", filter_type=filter_type),
        expand(intermediate_dir+"/{filter_type}/{graph_type}_{tax_level}.svg", 
                tax_level=tax_level,
                filter_type=filter_type,
                graph_type=graph_type),
        report("config/config.yaml", category="Configuration", labels = {
            "File" : "Parameters"
        }),
        report("resources/databases/"+config["emu"]["db"]+"/README_"+config["emu"]["db"]+".txt", category="Configuration", labels = {
            "File" : "Database"
        })
        
    wildcard_constraints: 
        graph_type='[a-zA-Z]+_[a-zA-Z]+',
        tax_level='[a-zA-Z]',
        filter_type='[a-zA-Z]',
        tax_type_counts='[a-zA-Z]'

    threads: 
        workflow.cores

rule initial_read_counts:
    input:
        input_dir+"/{id}.fastq"
    output: 
        intermediate_dir+"/temp/{id}_counts_0.csv"
    shell:
        "./workflow/scripts-sm/count_fastq.sh {input} {output}"

rule fastqc_pre:
    input: 
        input_dir+"/{id}.fastq"
    output: 
        report(intermediate_dir+"/preqc/{id}_fastqc.html", category="Before QC",
        labels = {
            "Sample" : "{id}"
        }),
        temp(intermediate_dir+"/preqc/{id}_fastqc.zip")
    log:
        intermediate_dir+"/logs/preqc/{id}_fastqc.log"
    threads: 
        workflow.cores
    shell:
        "workflow/scripts/fastqc_v0.11.7/fastqc -o "+intermediate_dir+"/preqc -t {workflow.cores} {input} &> {log}"

rule porechop:
    input: 
        input_dir+"/{id}.fastq"
    output:
        temp(intermediate_dir+"/postqc/{id}.porechop.fastq")
    log:
        intermediate_dir+"/logs/postqc/{id}_porechop.log"
    threads: 
        workflow.cores
    shell:
        "porechop -t {workflow.cores} -i {input} -o {output} &> {log}"

rule porechop_read_counts:
    input:
        intermediate_dir+"/postqc/{id}.porechop.fastq"
    output: 
        intermediate_dir+"/temp/{id}_counts_1.csv"
    shell:
        "./workflow/scripts-sm/count_fastq.sh {input} {output}"
        
rule nanofilt:
    input:
        intermediate_dir+"/postqc/{id}.porechop.fastq"
    output:
        temp(intermediate_dir+"/postqc/{id}.porechop.nanofilt.fastq")
    params:
        min_len = config["nanofilt"]["min_len"],
        max_len = config["nanofilt"]["max_len"],
        quality = config["nanofilt"]["quality"],
        min_gc = config["nanofilt"]["min_gc"],
        max_gc = config["nanofilt"]["max_gc"],
        head_crop = config["nanofilt"]["head_crop"],
        tail_crop = config["nanofilt"]["tail_crop"]
    wildcard_constraints:
        id='\w+'
    log:
        intermediate_dir+"/logs/postqc/{id}_nanofilt.log"
    threads: 
        workflow.cores
    run:
        parameters = "-l {params.min_len} --maxlength {params.max_len}"
        if ({params.quality} != 0):
            parameters += " --q {params.quality}"
        if ({params.min_gc} != 0.0):
            parameters += " --minGC {params.min_gc}"
        if ({params.max_gc} != 1.0):
            parameters += " --maxGC {params.max_gc}"
        if ({params.head_crop} != 0):
            parameters += " --headcrop {params.head_crop}"
        if ({params.tail_crop} != 0):
            parameters += " --tailcrop {params.tail_crop}"
        shell("NanoFilt --logfile {log} " + parameters + " {input} > {output} ")


rule nanofilt_read_counts:
    input:
        intermediate_dir+"/postqc/{id}.porechop.nanofilt.fastq"
    output: 
        intermediate_dir+"/temp/{id}_counts_2.csv"
    shell:
        "./workflow/scripts-sm/count_fastq.sh {input} {output}"

rule minimap:
    input:
        intermediate_dir+"/postqc/{id}.porechop.nanofilt.fastq"
    output:
        temp(intermediate_dir+"/postqc/{id}.porechop.nanofilt.paf")
    log:
        intermediate_dir+"/logs/postqc/{id}_minimap.log"
    threads: 
        workflow.cores
    shell: 
        "minimap2 -x ava-ont -g 500 -t {workflow.cores} {input} {input} > {output} 2> {log}"

rule yacrd:
    input:
        intermediate_dir+"/postqc/{id}.porechop.nanofilt.paf",
        intermediate_dir+"/postqc/{id}.porechop.nanofilt.fastq"
    output:
        temp(intermediate_dir+"/postqc/{id}.yacrd"),
        intermediate_dir+"/postqc/{id}_results_nanobird/{id}.fastq"
    log:
        intermediate_dir+"/logs/postqc/{id}_yacrd"
    wildcard_constraints:
        id='\w+'
    threads: 
        workflow.cores
    shell:
        "yacrd -t {workflow.cores} -i {input[0]} -o {output[0]} -c 4 -n 0.4 scrubb -i {input[1]} -o {output[1]} &> {log}"  

rule yacrd_read_counts:
    input:
        intermediate_dir+"/postqc/{id}_results_nanobird/{id}.fastq"
    output: 
        intermediate_dir+"/temp/{id}_counts_3.csv"
    shell:
        "./workflow/scripts-sm/count_fastq.sh {input} {output}"

rule aggregate_count_stats:
    input: 
        expand(intermediate_dir+"/temp/{id}_counts_{index}.csv", id=samples, index=[0,1,2,3])
    output:
        intermediate_dir+"/temp/{id}_read_counts.txt"
    run:
        sorted_input = sorted({input})
        current_sample = ""
        for file_input in sorted_input:
            for file2 in file_input:
                file_name = str(file2)
                if file_name.endswith("_counts_0.csv"):
                    sample_name = file_name.rstrip("_counts_0.csv")
                    sample_name = sample_name.lstrip(intermediate_dir+"/temp/")
                    current_sample = sample_name
                    with open(intermediate_dir+"/temp/"+sample_name+"_read_counts.txt", "w") as new_file:
                        with open(file_name, "r") as og_file:
                            for line in og_file:
                                new_file.write(line)
                elif file_name.endswith(".csv"):
                    with open(intermediate_dir+"/temp/"+current_sample+"_read_counts.txt", "a") as current_file:
                        with open(file_name, "r") as og_file:
                            for line in og_file:
                                current_file.write(line)
                
        
rule fastqc_post:
    input: 
        intermediate_dir+"/postqc/{id}_results_nanobird/{id}.fastq"
    output: 
        report(intermediate_dir+"/postqc/{id}_fastqc.html", category="After QC",
        labels = {
            "Sample" : "{id}"
        }),
        temp(intermediate_dir+"/postqc/{id}_fastqc.zip")
    log:
        intermediate_dir+"/logs/postqc/{id}_fastqc.log"
    threads: 
        workflow.cores
    shell:
        "workflow/scripts/fastqc_v0.11.7/fastqc -o "+intermediate_dir+"/postqc -t {workflow.cores} {input} &> {log}"

rule emu:
    input:
        intermediate_dir+"/postqc/{id}_results_nanobird/{id}.fastq"
    output:
        intermediate_dir+"/abundances/{id}_rel-abundance.tsv"
    log:
        intermediate_dir+"/logs/emu/{id}_emu"
    params:
        db = config["emu"]["db"]
    threads: 
        workflow.cores
    shell:
        "emu abundance --keep-counts --type map-ont --db resources/databases/{params.db} --output-dir "+intermediate_dir+"/abundances --threads {workflow.cores} {input} &> {log}"

rule combine_outputs:
    input:
        expand(intermediate_dir+"/abundances/{id}_rel-abundance.tsv", id=samples)
    output:
        intermediate_dir+"/emu-combined-tax_id-counts.tsv"
    log:
        intermediate_dir+"/logs/combine_outputs.log"
    shell:
        """
        emu combine-outputs """+intermediate_dir+"""/abundances tax_id --counts > {log}
        mv """+intermediate_dir+"""/abundances/emu-combined-tax_id-counts.tsv """+intermediate_dir+"""/emu-combined-tax_id-counts.tsv
        """

rule filter_count_tables:
    input:
        intermediate_dir+"/emu-combined-tax_id-counts.tsv"
    output:
        report(intermediate_dir+"/formated_emu_table_combined_counts_{filter_type}.tsv", category="Tables", subcategory="Count tables", labels=dict_labels_filter_type)
    log:
        intermediate_dir+"/logs/{filter_type}_count_tables.log"
    shell:  
        "python workflow/scripts/format_count_tables.py -t {input} -o "+intermediate_dir

rule relative_count_tables:
    input:
        intermediate_dir+"/formated_emu_table_combined_counts_{filter_type}.tsv"
    output:
        report(intermediate_dir+"/counts_{filter_type}_{tax_type_counts}.tsv", category="Tables", subcategory="Percentage tables", labels=dict_labels_filter_tax)
    log:
        intermediate_dir+"/logs/{filter_type}_{tax_type_counts}_relative_count_tables.log"
    shell:  
        """
        python workflow/scripts-sm/relative_count_tables.py {input}
        mv """+intermediate_dir+"""/formated_emu_table_combined_counts_{wildcards.filter_type}_{wildcards.tax_type_counts}.tsv """+intermediate_dir+"""/counts_{wildcards.filter_type}_{wildcards.tax_type_counts}.tsv
        """ 

rule complete_count_stats:
    input:
        intermediate_dir+"/formated_emu_table_combined_counts_filtered.tsv",
        expand(intermediate_dir+"/temp/{id}_read_counts.txt", id=samples)
    output: 
        report(intermediate_dir+"/read_counts.xlsx", category="Tables", subcategory="Stats", labels={
            "Type": "Stats"
        })
    shell:
        """
        python workflow/scripts/write_global_stats.py -d """+intermediate_dir+"""/temp -t {input[0]}
        mv """+intermediate_dir+"""/temp/read_counts.xlsx """+intermediate_dir+"""/read_counts.xlsx
        """
rule kronas:
    input: 
        intermediate_dir+"/formated_emu_table_combined_counts_{filter_type}.tsv"
    output:
        report(intermediate_dir+"/krona_{filter_type}/krona_composition_samples.html",
            category="Kronas",
            labels=dict_labels_filter_type
        )
    log:
        intermediate_dir+"/logs/kronas_{filter_type}.log"
    shell:  
        "python workflow/scripts/table_2_krona.py -t {input} -o " + intermediate_dir + "/krona_{wildcards.filter_type}"

rule plots:
    input:
        intermediate_dir+"/formated_emu_table_combined_counts_unfiltered.tsv"
    output:
        report(
            expand(
                intermediate_dir+"/{{filter_type}}/{{graph_type}}_{{tax_level}}.svg"),
            category=lambda wildcards: 'Top taxa plots {filter_type}'.format(filter_type=wildcards.filter_type),
            subcategory=lambda wildcards: '{tax_level}'.format(tax_level=wildcards.tax_level).capitalize(),
            labels=dict_labels
        )

    log: 
        intermediate_dir+"/logs/plots/{filter_type}_{graph_type}_{tax_level}"
    params:
        max_num_taxa = config["plots"]["max_num_taxa"],
        min_num_reads = config["plots"]["min_num_reads"]
    shell:
        """
        python workflow/scripts/plots.py -t {input} -d """+intermediate_dir+""" -n {params.max_num_taxa} -r {params.min_num_reads} &> {log}
        """

onstart:
    input_files = [f for f in os.listdir(input_dir) if (os.path.isfile(os.path.join(input_dir, f)) and f.endswith('fastq'))]
    input_names = [x[:-6] for x in input_files]
    if os.path.exists(intermediate_dir+"/preqc"):
        preqc_files = [f for f in os.listdir(intermediate_dir+"/preqc") if os.path.isfile(os.path.join(intermediate_dir+"/preqc", f))]
        preqc_names = [x[:-12] for x in preqc_files]
        for candidate in list(set(preqc_names) - set(input_names)):
            try: 
                os.remove(intermediate_dir+"/preqc/"+candidate+"_fastqc.html")
            except OSError:
                pass
            try: 
                os.remove(intermediate_dir+"/preqc/"+candidate+"_fastqc.zip")
            except OSError:
                pass
    if os.path.exists(intermediate_dir+"/postqc"):
        postqc_files = [f for f in os.listdir(intermediate_dir+"/postqc") if (os.path.isfile(os.path.join(intermediate_dir+"/postqc", f)) and f.endswith('html'))]
        postqc_names = [x[:-12] for x in postqc_files]
        for candidate in list(set(postqc_names) - set(input_names)):
            try: 
                os.remove(intermediate_dir+"/postqc/"+candidate+"_fastqc.html")
            except OSError:
                pass
            try: 
                os.remove(intermediate_dir+"/postqc/"+candidate+"_fastqc.zip")
            except OSError:
                pass
            try: 
                shutil.rmtree(intermediate_dir+"/postqc/"+candidate+"_results_nanobird",ignore_errors=True)
            except OSError:
                print(OSError)
    if os.path.exists(intermediate_dir+"/krona_filtered"):
        krona_filtered_files = [f for f in os.listdir(intermediate_dir+"/krona_filtered") if (os.path.isfile(os.path.join(intermediate_dir+"/krona_filtered", f)) and f.endswith('txt'))]
        krona_filtered_names = [x[:-4] for x in krona_filtered_files]
        for candidate in list(set(krona_filtered_names) - set(input_names)):
            try: 
                os.remove(intermediate_dir+"/krona_filtered/"+candidate+".txt")
            except OSError:
                pass
    if os.path.exists(intermediate_dir+"/krona_unfiltered"):
        krona_unfiltered_files = [f for f in os.listdir(intermediate_dir+"/krona_unfiltered") if (os.path.isfile(os.path.join(intermediate_dir+"/krona_unfiltered", f)) and f.endswith('txt'))]
        krona_unfiltered_names = [x[:-4] for x in krona_filtered_files]
        for candidate in list(set(krona_unfiltered_names) - set(input_names)):
            try: 
                os.remove(intermediate_dir+"/krona_unfiltered/"+candidate+".txt")
            except OSError:
                pass
        
    if os.path.exists(intermediate_dir):
        results_files = [f for f in os.listdir(intermediate_dir) if (os.path.isfile(os.path.join(intermediate_dir, f)) and f.endswith('txt'))]
        results_names = [x[:-4] for x in results_files]
    if os.path.exists(intermediate_dir+"/abundances"):
        abundances_files = [f for f in os.listdir(intermediate_dir+"/abundances") if os.path.isfile(os.path.join(intermediate_dir+"/abundances", f))]
        abundances_names = [x[:-18] for x in abundances_files]
        for candidate in list(set(abundances_names) - set(input_names)):
            try: 
                os.remove(intermediate_dir+"/abundances"+"/"+candidate+"_rel-abundance.tsv")
            except OSError:
                pass
    if os.path.exists(intermediate_dir+"/temp"):
        temp_files = [f for f in os.listdir(intermediate_dir+"/temp") if (os.path.isfile(os.path.join(intermediate_dir+"/temp", f)) and f.endswith('counts_0.csv'))]
        temp_names = [x[:-13] for x in temp_files]
        for candidate in list(set(temp_names) - set(input_names)):
            try: 
                os.remove(intermediate_dir+"/temp/"+candidate+"_counts_0.csv")
            except OSError:
                pass
            try: 
                os.remove(intermediate_dir+"/temp/"+candidate+"_counts_1.csv")
            except OSError:
                pass
            try: 
                os.remove(intermediate_dir+"/temp/"+candidate+"_counts_2.csv")
            except OSError:
                pass
            try: 
                os.remove(intermediate_dir+"/temp/"+candidate+"_counts_3.csv")
            except OSError:
                pass
            try: 
                os.remove(intermediate_dir+"/temp/"+candidate+"_read_counts.txt")
            except OSError:
                pass
    
    if (sum(len(files) for _, _, files in os.walk(input_dir)) == 1):
        if os.path.exists(intermediate_dir+"/filtered"):
            heatmap_files_filt = [f for f in os.listdir(intermediate_dir+"/filtered") if (os.path.isfile(os.path.join(intermediate_dir+"/filtered", f)) and f.startswith('clustered'))]
            for candidate in heatmap_files_filt:
                try: 
                    os.remove(intermediate_dir+"/filtered/"+candidate)
                except OSError:
                    pass
        if os.path.exists(intermediate_dir+"/unfiltered"):
            heatmap_files_unfilt = [f for f in os.listdir(intermediate_dir+"/unfiltered") if (os.path.isfile(os.path.join(intermediate_dir+"/unfiltered", f)) and f.startswith('clustered'))]
            for candidate in heatmap_files_unfilt:
                try: 
                    os.remove(intermediate_dir+"/unfiltered/"+candidate)
                except OSError:
                    pass
    os.makedirs(output_dir, exist_ok=True)

onsuccess:
    from snakemake.report import auto_report 
    auto_report(workflow.persistence.dag, output_dir+"/report.html", stylesheet="workflow/report/custom-stylesheet.css")